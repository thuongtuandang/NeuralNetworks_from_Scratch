# NeuralNetworks_from_Scratch

Here is a repository designed for educational purposes, focusing on building neural networks from scratch for Natural Language Processing (NLP) tasks. This repository includes two main models:

- LSTM Model: This model is designed to predict the next words in a sequence, showcasing a practical application of LSTM (Long Short-Term Memory) networks in language modeling. Within the LSTM folder, the documentation provides a detailed explanation of the model's architecture. It also includes step-by-step calculations and sample Python code snippets to illustrate how the model is constructed and how it functions.

- Transformers Model: This model aims at sentiment prediction, demonstrating the use of transformer architectures in understanding and interpreting the sentiment of text inputs. Similar to the LSTM model, the transformers folder contains comprehensive documentation that explains the architecture of the transformer model. It also walks through the calculations involved and provides sample Python code to help learners understand the implementation details and the operational mechanics of transformers in sentiment analysis tasks.

Both sections are crafted to facilitate a deep understanding of these advanced neural network architectures. The documentation in each folder not only elucidates the theoretical aspects of LSTM and transformer models but also bridges theory with practical application through sample code. This approach ensures that learners can grasp the concepts and apply them to real-world NLP tasks.  